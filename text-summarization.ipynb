{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shahjaysuhasbhai/text-summarizatio?scriptVersionId=273131298\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:18:28.847479Z","iopub.execute_input":"2025-11-03T12:18:28.84781Z","iopub.status.idle":"2025-11-03T12:18:31.412874Z","shell.execute_reply.started":"2025-11-03T12:18:28.84778Z","shell.execute_reply":"2025-11-03T12:18:31.411803Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:18:33.086545Z","iopub.execute_input":"2025-11-03T12:18:33.086898Z","iopub.status.idle":"2025-11-03T12:18:40.868937Z","shell.execute_reply.started":"2025-11-03T12:18:33.086875Z","shell.execute_reply":"2025-11-03T12:18:40.867539Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:19:00.12451Z","iopub.execute_input":"2025-11-03T12:19:00.124863Z","iopub.status.idle":"2025-11-03T12:19:15.572842Z","shell.execute_reply.started":"2025-11-03T12:19:00.124838Z","shell.execute_reply":"2025-11-03T12:19:15.571597Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"1. AutoTokenizer: class from the Hugging Face Transformers library\n2. from_pretrained(model_name): downloads particular tokenizer identified by model_name\n3. AutoModelForSeq2SeqLM: class from Hugging Face’s transformers library designed for sequence-to-sequence tasks, such as translation, text summarization, etc\n4. from_pretrained(model_name): downloads the model identified by model_name with **weights and architecture** of it.","metadata":{}},{"cell_type":"code","source":"model_name = \"facebook/bart-large-cnn\"  # or \"t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:19:46.38789Z","iopub.execute_input":"2025-11-03T12:19:46.388452Z","iopub.status.idle":"2025-11-03T12:20:18.811917Z","shell.execute_reply.started":"2025-11-03T12:19:46.388423Z","shell.execute_reply":"2025-11-03T12:20:18.810629Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"087769e4d2c9490c83660bc481786874"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb2d43f1e1104c2f9a30f581d0e09d18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab5d332cbfb641d3a1893e5abae75c49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67980dc77b2d40fa9abff49da0f47c44"}},"metadata":{}},{"name":"stderr","text":"2025-11-03 12:19:52.815813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762172393.096959      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762172393.193926      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4431e00cdc9647f2974705a1ed31ff0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e05d6b5428c416b95bf5815af752940"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"     inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n\n1. tokenizer: converts raw text into token representations\n2. return_tensors=\"pt\": output will be in pytorch tensors as model should require tensors, not tokens-strings\n3. max_length=max_input_length, Here, 1024 tokens\n4. truncation = True means if text has more than 1024 tokens then it truncates them","metadata":{}},{"cell_type":"code","source":"# takes input text, encodes and summarizes it.\ndef summarize(text, model, tokenizer, max_input_length=1024, max_output_length=100):\n    #inputs contains text with it's corresponding vectored representation for each tokens\n    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n    summary_ids = model.generate(\n        inputs[\"input_ids\"], # input_ids having vectored/tensor representation\n        num_beams=4, \n        max_length=max_output_length, \n        early_stopping=True #  generation process will stop once the beam search finds a complete hypothesis for all beams, not continuing generation unnecessarily \n    )\n    # summary_ids[0] contains token_ids generated by model, we have to decode it\n    # skip_special_tokens=True will skip special tokens like <b> <s> > ? / etc\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True) \n    return summary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:56:19.520913Z","iopub.execute_input":"2025-11-03T12:56:19.521412Z","iopub.status.idle":"2025-11-03T12:56:19.528868Z","shell.execute_reply.started":"2025-11-03T12:56:19.521385Z","shell.execute_reply":"2025-11-03T12:56:19.52764Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"**num_beams=4** : Beam search is a text generation algorithm that tries to find the most likely output sequence by exploring multiple candidate sequences simultaneously.\n1. with num_beams=4, at each generation step the algorithm keeps the 4 most promising candidate sequences.\n2. When num_beams=1, beam search reduces to greedy search, only considering the single best next token at every step.","metadata":{}},{"cell_type":"code","source":"article = \"\"\"\nA hungry crow found a piece of bread and flew away with it. As the crow settled on a tree branch, \na clever fox saw the bread and desired it. The fox praised the crow for its beautiful wings and sweet voice, \nflattering it to make the crow sing. Wanting to impress the fox, the crow opened its beak to caw, dropping the bread.\nThe fox quickly grabbed the bread and disappeared into the woods, leaving the crow to reflect on its mistake.\n\"\"\"\nsummary_result = summarize(article, model, tokenizer)\nprint(summary_result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:56:21.000808Z","iopub.execute_input":"2025-11-03T12:56:21.001829Z","iopub.status.idle":"2025-11-03T12:56:36.815068Z","shell.execute_reply.started":"2025-11-03T12:56:21.001798Z","shell.execute_reply":"2025-11-03T12:56:36.813955Z"}},"outputs":[{"name":"stdout","text":"A fox praised the crow for its beautiful wings and sweet voice. Wanting to impress the fox, the crow opened its beak to caw, dropping the bread. The fox quickly grabbed the bread and disappeared into the woods, leaving the crow to reflect on its mistake.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}